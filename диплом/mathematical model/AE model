
无监督语言模型分成两类，
一类是AR自回归模型，如GPT、ELMo这种使用单向语言模型建模概率分布的方法；
另一类是AE自编码模型，如BERT这种通过预测句子中token的方法。
XLNet融合了AR模型和AE模型各自的优点，既能建模概率密度，适用于文本生成类任务，又能充分使用双向上下文信息。
XLNet实现AR和AE融合的主要思路为，对输入文本进行排列组合，然后对于每个排列组合使用AR的方式训练，不同排列组合使每个token都能和其他token进行信息交互，同时每次训练又都是AR的。
但是，实现这种模式也存在很多问题，文中针对这些问题提出了解法，由于篇幅原因具体内容我们会在后续相关文章再进行详细介绍

[自动编码AE器](https://github.com/zangzelin/Auto-encoder-AE-SAE-DAE-CAE-DAE-with-keras-in-Mnist-and-report/blob/master/Report.md#21-%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8auto-encodersae)
[AR自回归模型](https://otexts.com/fppcn/AR.html)
